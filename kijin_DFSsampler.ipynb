{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# NeighborSampler"],"metadata":{"id":"-zNVnChpb3R7"}},{"cell_type":"code","source":["\"\"\"Data loading components for neighbor sampling\"\"\"\n","from ..base import NID, EID\n","from ..transforms import to_block\n","from .base import BlockSampler\n","\n","import time\n","\n","class NeighborSampler(BlockSampler):\n","    \"\"\"Sampler that builds computational dependency of node representations via\n","    neighbor sampling for multilayer GNN.\n","\n","    This sampler will make every node gather messages from a fixed number of neighbors\n","    per edge type.  The neighbors are picked uniformly.\n","\n","    Parameters\n","    ----------\n","    fanouts : list[int] or list[dict[etype, int]]\n","        List of neighbors to sample per edge type for each GNN layer, with the i-th\n","        element being the fanout for the i-th GNN layer.\n","\n","        If only a single integer is provided, DGL assumes that every edge type\n","        will have the same fanout.\n","\n","        If -1 is provided for one edge type on one layer, then all inbound edges\n","        of that edge type will be included.\n","    edge_dir : str, default ``'in'``\n","        Can be either ``'in' `` where the neighbors will be sampled according to\n","        incoming edges, or ``'out'`` otherwise, same as :func:`dgl.sampling.sample_neighbors`.\n","    prob : str, optional\n","        If given, the probability of each neighbor being sampled is proportional\n","        to the edge feature value with the given name in ``g.edata``.  The feature must be\n","        a scalar on each edge.\n","\n","        This argument is mutually exclusive with :attr:`mask`.  If you want to\n","        specify both a mask and a probability, consider multiplying the probability\n","        with the mask instead.\n","    mask : str, optional\n","        If given, a neighbor could be picked only if the edge mask with the given\n","        name in ``g.edata`` is True.  The data must be boolean on each edge.\n","\n","        This argument is mutually exclusive with :attr:`prob`.  If you want to\n","        specify both a mask and a probability, consider multiplying the probability\n","        with the mask instead.\n","    replace : bool, default False\n","        Whether to sample with replacement\n","    prefetch_node_feats : list[str] or dict[ntype, list[str]], optional\n","        The source node data to prefetch for the first MFG, corresponding to the\n","        input node features necessary for the first GNN layer.\n","    prefetch_labels : list[str] or dict[ntype, list[str]], optional\n","        The destination node data to prefetch for the last MFG, corresponding to\n","        the node labels of the minibatch.\n","    prefetch_edge_feats : list[str] or dict[etype, list[str]], optional\n","        The edge data names to prefetch for all the MFGs, corresponding to the\n","        edge features necessary for all GNN layers.\n","    output_device : device, optional\n","        The device of the output subgraphs or MFGs.  Default is the same as the\n","        minibatch of seed nodes.\n","\n","    Examples\n","    --------\n","    **Node classification**\n","\n","    To train a 3-layer GNN for node classification on a set of nodes ``train_nid`` on\n","    a homogeneous graph where each node takes messages from 5, 10, 15 neighbors for\n","    the first, second, and third layer respectively (assuming the backend is PyTorch):\n","\n","    >>> sampler = dgl.dataloading.NeighborSampler([5, 10, 15])\n","    >>> dataloader = dgl.dataloading.DataLoader(\n","    ...     g, train_nid, sampler,\n","    ...     batch_size=1024, shuffle=True, drop_last=False, num_workers=4)\n","    >>> for input_nodes, output_nodes, blocks in dataloader:\n","    ...     train_on(blocks)\n","\n","    If training on a heterogeneous graph and you want different number of neighbors for each\n","    edge type, one should instead provide a list of dicts.  Each dict would specify the\n","    number of neighbors to pick per edge type.\n","\n","    >>> sampler = dgl.dataloading.NeighborSampler([\n","    ...     {('user', 'follows', 'user'): 5,\n","    ...      ('user', 'plays', 'game'): 4,\n","    ...      ('game', 'played-by', 'user'): 3}] * 3)\n","\n","    If you would like non-uniform neighbor sampling:\n","\n","    >>> g.edata['p'] = torch.rand(g.num_edges())   # any non-negative 1D vector works\n","    >>> sampler = dgl.dataloading.NeighborSampler([5, 10, 15], prob='p')\n","\n","    Or sampling on edge masks:\n","\n","    >>> g.edata['mask'] = torch.rand(g.num_edges()) < 0.2   # any 1D boolean mask works\n","    >>> sampler = dgl.dataloading.NeighborSampler([5, 10, 15], prob='mask')\n","\n","    **Edge classification and link prediction**\n","\n","    This class can also work for edge classification and link prediction together\n","    with :func:`as_edge_prediction_sampler`.\n","\n","    >>> sampler = dgl.dataloading.NeighborSampler([5, 10, 15])\n","    >>> sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n","    >>> dataloader = dgl.dataloading.DataLoader(\n","    ...     g, train_eid, sampler,\n","    ...     batch_size=1024, shuffle=True, drop_last=False, num_workers=4)\n","\n","    See the documentation :func:`as_edge_prediction_sampler` for more details.\n","\n","    Notes\n","    -----\n","    For the concept of MFGs, please refer to\n","    :ref:`User Guide Section 6 <guide-minibatch>` and\n","    :doc:`Minibatch Training Tutorials <tutorials/large/L0_neighbor_sampling_overview>`.\n","    \"\"\"\n","    def __init__(self, fanouts, edge_dir='in', prob=None, mask=None, replace=False,\n","                 prefetch_node_feats=None, prefetch_labels=None, prefetch_edge_feats=None,\n","                 output_device=None):\n","        super().__init__(prefetch_node_feats=prefetch_node_feats,\n","                         prefetch_labels=prefetch_labels,\n","                         prefetch_edge_feats=prefetch_edge_feats,\n","                         output_device=output_device)\n","        self.fanouts = fanouts\n","        self.edge_dir = edge_dir\n","        if mask is not None and prob is not None:\n","            raise ValueError(\n","                    'Mask and probability arguments are mutually exclusive. '\n","                    'Consider multiplying the probability with the mask '\n","                    'to achieve the same goal.')\n","        self.prob = prob or mask\n","        self.replace = replace\n","\n","    def sample_blocks(self, g, seed_nodes, exclude_eids=None):\n","\n","        start = time.time()\n","        sample_time = 0\n","\n","        output_nodes = seed_nodes\n","        blocks = []\n","        for fanout in reversed(self.fanouts):\n","            frontier = g.sample_neighbors(\n","                seed_nodes, fanout, edge_dir=self.edge_dir, prob=self.prob,\n","                replace=self.replace, output_device=self.output_device,\n","                exclude_edges=exclude_eids)\n","            eid = frontier.edata[EID]\n","            block = to_block(frontier, seed_nodes)\n","            block.edata[EID] = eid\n","            seed_nodes = block.srcdata[NID]\n","            blocks.insert(0, block)\n","\n","        end = time.time()\n","        sample_time = end-start\n","        # print(\"{:.4f}\".format(sample_time))\n","\n","        return seed_nodes, output_nodes, blocks\n","\n","class NeighborDFSampler(BlockSampler):\n","\n","    def __init__(self, fanouts, edge_dir='in', prob=None, mask=None, replace=False,\n","                 prefetch_node_feats=None, prefetch_labels=None, prefetch_edge_feats=None,\n","                 output_device=None):\n","        super().__init__(prefetch_node_feats=prefetch_node_feats,\n","                         prefetch_labels=prefetch_labels,\n","                         prefetch_edge_feats=prefetch_edge_feats,\n","                         output_device=output_device)\n","        self.fanouts = fanouts\n","        self.edge_dir = edge_dir\n","        if mask is not None and prob is not None:\n","            raise ValueError(\n","                    'Mask and probability arguments are mutually exclusive. '\n","                    'Consider multiplying the probability with the mask '\n","                    'to achieve the same goal.')\n","        self.prob = prob or mask\n","        self.replace = replace\n","\n","    def sample_blocks(self, g, seed_nodes, exclude_eids=None):\n","\n","        start = time.time()\n","        sample_time = 0\n","\n","        output_nodes = seed_nodes\n","        blocks = []\n","        # g must be DisGraph instance\n","\n","        frontiers = g.df_sample_neighbors(\n","            seed_nodes, self.fanouts, edge_dir=self.edge_dir, prob=self.prob,\n","            replace=self.replace, output_device=self.output_device,\n","            exclude_edges=exclude_eids)\n","\n","        for frontier in frontiers:\n","            eid = frontier.edata[EID]\n","            block = to_block(frontier, seed_nodes)\n","            block.edata[EID] = eid\n","            seed_nodes = block.srcdata[NID]\n","            blocks.insert(0, block)\n","\n","        end = time.time()\n","        sample_time = end-start\n","        # print(\"{:.4f}\".format(sample_time))\n","\n","        return seed_nodes, output_nodes, blocks\n","\n","MultiLayerNeighborSampler = NeighborSampler\n","\n","class MultiLayerFullNeighborSampler(NeighborSampler):\n","    \"\"\"Sampler that builds computational dependency of node representations by taking messages\n","    from all neighbors for multilayer GNN.\n","\n","    This sampler will make every node gather messages from every single neighbor per edge type.\n","\n","    Parameters\n","    ----------\n","    n_layers : int\n","        The number of GNN layers to sample.\n","    kwargs :\n","        Passed to :class:`dgl.dataloading.NeighborSampler`.\n","\n","    Examples\n","    --------\n","    To train a 3-layer GNN for node classification on a set of nodes ``train_nid`` on\n","    a homogeneous graph where each node takes messages from all neighbors for the first,\n","    second, and third layer respectively (assuming the backend is PyTorch):\n","\n","    >>> sampler = dgl.dataloading.MultiLayerFullNeighborSampler(3)\n","    >>> dataloader = dgl.dataloading.DataLoader(\n","    ...     g, train_nid, sampler,\n","    ...     batch_size=1024, shuffle=True, drop_last=False, num_workers=4)\n","    >>> for input_nodes, output_nodes, blocks in dataloader:\n","    ...     train_on(blocks)\n","\n","    Notes\n","    -----\n","    For the concept of MFGs, please refer to\n","    :ref:`User Guide Section 6 <guide-minibatch>` and\n","    :doc:`Minibatch Training Tutorials <tutorials/large/L0_neighbor_sampling_overview>`.\n","    \"\"\"\n","    def __init__(self, num_layers, **kwargs):\n","        super().__init__([-1] * num_layers, **kwargs)\n"],"metadata":{"id":"3bM6_jjeb0MZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# graph_services"],"metadata":{"id":"lULv36FccWOq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UetM5JcL7ktR"},"outputs":[],"source":["\"\"\"A set of graph services of getting subgraphs from DistGraph\"\"\"\n","from collections import namedtuple\n","\n","import numpy as np\n","import time\n","\n","from .. import backend as F\n","from ..base import EID, NID\n","from ..convert import graph, heterograph\n","from ..sampling import sample_etype_neighbors as local_sample_etype_neighbors\n","from ..sampling import sample_neighbors as local_sample_neighbors\n","from ..subgraph import in_subgraph as local_in_subgraph\n","from ..utils import toindex\n","from .. import backend as F\n","from .rpc import (\n","    Request,\n","    Response,\n","    recv_responses,\n","    register_service,\n","    send_requests_to_machine,\n",")\n","\n","__all__ = [\n","    \"sample_neighbors\",\n","    \"sample_etype_neighbors\",\n","    \"in_subgraph\",\n","    \"find_edges\",\n","]\n","\n","SAMPLING_SERVICE_ID = 6657\n","INSUBGRAPH_SERVICE_ID = 6658\n","EDGES_SERVICE_ID = 6659\n","OUTDEGREE_SERVICE_ID = 6660\n","INDEGREE_SERVICE_ID = 6661\n","ETYPE_SAMPLING_SERVICE_ID = 6662\n","DFSAMPLING_SERVICE_ID = 6666\n","\n","class DFSubgraphResponse(Response):\n","    \"\"\"The response for sampling and in_subgraph\"\"\"\n","\n","    def __init__(self, LocalSampledGraphs, bucket):\n","        self.LocalSampledGraphs = LocalSampledGraphs\n","        self.bucket = bucket\n","\n","    def __setstate__(self, state):\n","        self.LocalSampledGraphs, self.bucket = state\n","\n","    def __getstate__(self):\n","        return self.LocalSampledGraphs, self.bucket\n","\n","class SubgraphResponse(Response):\n","    \"\"\"The response for sampling and in_subgraph\"\"\"\n","\n","    def __init__(self, global_src, global_dst, global_eids):\n","        self.global_src = global_src\n","        self.global_dst = global_dst\n","        self.global_eids = global_eids\n","\n","    def __setstate__(self, state):\n","        self.global_src, self.global_dst, self.global_eids = state\n","\n","    def __getstate__(self):\n","        return self.global_src, self.global_dst, self.global_eids\n","\n","class FindEdgeResponse(Response):\n","    \"\"\"The response for sampling and in_subgraph\"\"\"\n","\n","    def __init__(self, global_src, global_dst, order_id):\n","        self.global_src = global_src\n","        self.global_dst = global_dst\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.global_src, self.global_dst, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.global_src, self.global_dst, self.order_id\n","\n","\n","def _sample_neighbors(\n","    local_g, partition_book, seed_nodes, fan_out, edge_dir, prob, replace\n","):\n","    \"\"\"Sample from local partition.\n","\n","    The input nodes use global IDs. We need to map the global node IDs to local node IDs,\n","    perform sampling and map the sampled results to the global IDs space again.\n","    The sampled results are stored in three vectors that store source nodes, destination nodes\n","    and edge IDs.\n","    \"\"\"\n","    local_ids = partition_book.nid2localnid(seed_nodes, partition_book.partid)\n","    local_ids = F.astype(local_ids, local_g.idtype)\n","    # local_ids = self.seed_nodes\n","    sampled_graph = local_sample_neighbors(\n","        local_g,\n","        local_ids,\n","        fan_out,\n","        edge_dir,\n","        prob,\n","        replace,\n","        _dist_training=True,\n","    )\n","    global_nid_mapping = local_g.ndata[NID]\n","    src, dst = sampled_graph.edges()\n","    global_src, global_dst = F.gather_row(\n","        global_nid_mapping, src\n","    ), F.gather_row(global_nid_mapping, dst)\n","    global_eids = F.gather_row(local_g.edata[EID], sampled_graph.edata[EID])\n","\n","    return global_src, global_dst, global_eids\n","\n","def _df_sample_neighbors(\n","    local_g, partition_book, seed_nodes, fan_outs, edge_dir, prob, replace, bucket, layer_idx\n","):\n","    # return items\n","    LocalSampledGraphs = []\n","    local_bucket = []\n","    fanouts_list = []\n","\n","    #fill local_bucket with empty list\n","    for i in range(len(fan_outs)-1):\n","      local_bucket.append([])\n","\n","    # control fanouts: bucket에 값이 존재하는 layer부터 sampling 수행\n","    if isinstance(fan_outs,int):\n","        fanouts_list.append(fan_outs)\n","    else:\n","        fanouts_list = fan_outs\n","\n","    sliced_fanouts = []\n","    for i in range(len(fanouts_list)-layer_idx):\n","        sliced_fanouts.append(fanouts_list[i])\n","    fan_outs = sliced_fanouts\n","\n","    start = time.time()\n","    for i, fan_out in enumerate(reversed(fan_outs)):\n","\n","        # seed_nodes를 local id로 매핑\n","        local_ids = partition_book.nid2localnid(seed_nodes, partition_book.partid)\n","        local_ids = F.astype(local_ids, local_g.idtype)\n","\n","        bucket_ids = None\n","        # bucket이 비어있지 않으면 값들을 seed_node에 추가\n","        if layer_idx > 0:\n","            if len(bucket[layer_idx+i-1])!=0:\n","                # bucket의 노드 중 현재 파티션에 속한 노드만 ids로 저장\n","\n","                bucket_ids = _check_local(partition_book, bucket[layer_idx+i-1])\n","\n","\n","        if bucket_ids is not None and i!=0:\n","            localized_bucket_ids = partition_book.nid2localnid(bucket_ids, partition_book.partid)\n","            localized_bucket_ids = F.astype(localized_bucket_ids, local_g.idtype)\n","            all_seed_nodes = []\n","            all_seed_nodes.extend(local_ids.tolist())\n","            all_seed_nodes.extend(localized_bucket_ids.tolist())\n","            all_seed_nodes = toindex(all_seed_nodes).tousertensor()\n","        else:\n","            all_seed_nodes = local_ids\n","\n","        # local sampling\n","        sampled_graph = local_sample_neighbors(\n","            local_g,\n","            all_seed_nodes,\n","            fan_out,\n","            edge_dir,\n","            prob,\n","            replace,\n","            _dist_training=True,\n","        )\n","\n","\n","        # local id를 다시 global로 매핑\n","        global_nid_mapping = local_g.ndata[NID]\n","        src, dst = sampled_graph.edges()\n","        global_src, global_dst = F.gather_row(\n","            global_nid_mapping, src\n","        ), F.gather_row(global_nid_mapping, dst)\n","        global_eids = F.gather_row(local_g.edata[EID], sampled_graph.edata[EID])\n","        # 샘플링 결과 저장\n","        LocalSampledGraphs.append(LocalSampledGraph(global_src, global_dst, global_eids))\n","\n","        # remote or local을 체크하여 local은 다시 seed_node에 remote는 bucket으로 삽입\n","        # local_nids는 local id이며 remote_list는 global id이다\n","        local_nids, remote_list = _check_location_local(local_g, partition_book, global_src)\n","        seed_nodes = local_nids\n","        if (layer_idx+i)<(len(fan_outs)-1) and len(remote_list)!=0:\n","          local_bucket[layer_idx+i].extend(remote_list)\n","\n","    end = time.time()\n","    f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/dfs_{}_loc_rem.txt\".format(\"1000\"),'a+')\n","    data = str(end-start)\n","    f.write(data+\"\\n\")\n","    f.close()\n","\n","    return LocalSampledGraphs, local_bucket\n","\n","def _sample_etype_neighbors(\n","    local_g,\n","    partition_book,\n","    seed_nodes,\n","    etype_offset,\n","    fan_out,\n","    edge_dir,\n","    prob,\n","    replace,\n","    etype_sorted=False,\n","):\n","    \"\"\"Sample from local partition.\n","\n","    The input nodes use global IDs. We need to map the global node IDs to local node IDs,\n","    perform sampling and map the sampled results to the global IDs space again.\n","    The sampled results are stored in three vectors that store source nodes, destination nodes\n","    and edge IDs.\n","    \"\"\"\n","    local_ids = partition_book.nid2localnid(seed_nodes, partition_book.partid)\n","    local_ids = F.astype(local_ids, local_g.idtype)\n","\n","    sampled_graph = local_sample_etype_neighbors(\n","        local_g,\n","        local_ids,\n","        etype_offset,\n","        fan_out,\n","        edge_dir,\n","        prob,\n","        replace,\n","        etype_sorted=etype_sorted,\n","        _dist_training=True,\n","    )\n","    global_nid_mapping = local_g.ndata[NID]\n","    src, dst = sampled_graph.edges()\n","    global_src, global_dst = F.gather_row(\n","        global_nid_mapping, src\n","    ), F.gather_row(global_nid_mapping, dst)\n","    global_eids = F.gather_row(local_g.edata[EID], sampled_graph.edata[EID])\n","    return global_src, global_dst, global_eids\n","\n","\n","def _find_edges(local_g, partition_book, seed_edges):\n","    \"\"\"Given an edge ID array, return the source\n","    and destination node ID array ``s`` and ``d`` in the local partition.\n","    \"\"\"\n","    local_eids = partition_book.eid2localeid(seed_edges, partition_book.partid)\n","    local_eids = F.astype(local_eids, local_g.idtype)\n","    local_src, local_dst = local_g.find_edges(local_eids)\n","    global_nid_mapping = local_g.ndata[NID]\n","    global_src = global_nid_mapping[local_src]\n","    global_dst = global_nid_mapping[local_dst]\n","    return global_src, global_dst\n","\n","\n","def _in_degrees(local_g, partition_book, n):\n","    \"\"\"Get in-degree of the nodes in the local partition.\"\"\"\n","    local_nids = partition_book.nid2localnid(n, partition_book.partid)\n","    local_nids = F.astype(local_nids, local_g.idtype)\n","    return local_g.in_degrees(local_nids)\n","\n","\n","def _out_degrees(local_g, partition_book, n):\n","    \"\"\"Get out-degree of the nodes in the local partition.\"\"\"\n","    local_nids = partition_book.nid2localnid(n, partition_book.partid)\n","    local_nids = F.astype(local_nids, local_g.idtype)\n","    return local_g.out_degrees(local_nids)\n","\n","\n","def _in_subgraph(local_g, partition_book, seed_nodes):\n","    \"\"\"Get in subgraph from local partition.\n","\n","    The input nodes use global IDs. We need to map the global node IDs to local node IDs,\n","    get in-subgraph and map the sampled results to the global IDs space again.\n","    The results are stored in three vectors that store source nodes, destination nodes\n","    and edge IDs.\n","    \"\"\"\n","    local_ids = partition_book.nid2localnid(seed_nodes, partition_book.partid)\n","    local_ids = F.astype(local_ids, local_g.idtype)\n","    # local_ids = self.seed_nodes\n","    sampled_graph = local_in_subgraph(local_g, local_ids)\n","    global_nid_mapping = local_g.ndata[NID]\n","    src, dst = sampled_graph.edges()\n","    global_src, global_dst = global_nid_mapping[src], global_nid_mapping[dst]\n","    global_eids = F.gather_row(local_g.edata[EID], sampled_graph.edata[EID])\n","    return global_src, global_dst, global_eids\n","\n","\n","# --- NOTE 1 ---\n","# (BarclayII)\n","# If the sampling algorithm needs node and edge data, ideally the\n","# algorithm should query the underlying feature storage to get what it\n","# just needs to complete the job.  For instance, with\n","# sample_etype_neighbors, we only need the probability of the seed nodes'\n","# neighbors.\n","#\n","# However, right now we are reusing the existing subgraph sampling\n","# interfaces of DGLGraph (i.e. single machine solution), which needs\n","# the data of *all* the nodes/edges.  Going distributed, we now need\n","# the node/edge data of the *entire* local graph partition.\n","#\n","# If the sampling algorithm only use edge data, the current design works\n","# because the local graph partition contains all the in-edges of the\n","# assigned nodes as well as the data.  This is the case for\n","# sample_etype_neighbors.\n","#\n","# However, if the sampling algorithm requires data of the neighbor nodes\n","# (e.g. sample_neighbors_biased which performs biased sampling based on the\n","# type of the neighbor nodes), the current design will fail because the\n","# neighbor nodes (hence the data) may not belong to the current partition.\n","# This is a limitation of the current DistDGL design.  We should improve it\n","# later.\n","\n","class SamplingRequest(Request):\n","    \"\"\"Sampling Request\"\"\"\n","\n","    def __init__(self, nodes, fan_out, edge_dir=\"in\", prob=None, replace=False):\n","        self.seed_nodes = nodes\n","        self.edge_dir = edge_dir\n","        self.prob = prob\n","        self.replace = replace\n","        self.fan_out = fan_out\n","\n","    def __setstate__(self, state):\n","        (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_out,\n","        ) = state\n","\n","    def __getstate__(self):\n","        return (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_out,\n","        )\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        kv_store = server_state.kv_store\n","        if self.prob is not None:\n","            prob = [kv_store.data_store[self.prob]]\n","        else:\n","            prob = None\n","        global_src, global_dst, global_eids = _sample_neighbors(\n","            local_g,\n","            partition_book,\n","            self.seed_nodes,\n","            self.fan_out,\n","            self.edge_dir,\n","            prob,\n","            self.replace,\n","        )\n","        return SubgraphResponse(global_src, global_dst, global_eids)\n","\n","class DFSamplingRequest(Request):\n","    \"\"\"Sampling Request\"\"\"\n","\n","    def __init__(self, nodes, fan_outs, edge_dir=\"in\", prob=None, replace=False, bucket=None, layer_idx = None):\n","        self.seed_nodes = nodes\n","        self.edge_dir = edge_dir\n","        self.prob = prob\n","        self.replace = replace\n","        self.fan_outs = fan_outs\n","        self.bucket = bucket\n","        self.layer_idx = layer_idx\n","\n","    def __setstate__(self, state):\n","        (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_outs,\n","            self.bucket,\n","            self.layer_idx\n","        ) = state\n","\n","    def __getstate__(self):\n","        return (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_outs,\n","            self.bucket,\n","            self.layer_idx\n","        )\n","\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        kv_store = server_state.kv_store\n","        if self.prob is not None:\n","            prob = [kv_store.data_store[self.prob]]\n","        else:\n","            prob = None\n","        LocalSampledGraphs, local_bucket = _df_sample_neighbors(\n","            local_g,\n","            partition_book,\n","            self.seed_nodes,\n","            self.fan_outs,\n","            self.edge_dir,\n","            prob,\n","            self.replace,\n","            self.bucket,\n","            self.layer_idx\n","        )\n","        return DFSubgraphResponse(LocalSampledGraphs, local_bucket)\n","\n","class SamplingRequestEtype(Request):\n","    \"\"\"Sampling Request\"\"\"\n","\n","    def __init__(\n","        self,\n","        nodes,\n","        fan_out,\n","        edge_dir=\"in\",\n","        prob=None,\n","        replace=False,\n","        etype_sorted=True,\n","    ):\n","        self.seed_nodes = nodes\n","        self.edge_dir = edge_dir\n","        self.prob = prob\n","        self.replace = replace\n","        self.fan_out = fan_out\n","        self.etype_sorted = etype_sorted\n","\n","    def __setstate__(self, state):\n","        (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_out,\n","            self.etype_sorted,\n","        ) = state\n","\n","    def __getstate__(self):\n","        return (\n","            self.seed_nodes,\n","            self.edge_dir,\n","            self.prob,\n","            self.replace,\n","            self.fan_out,\n","            self.etype_sorted,\n","        )\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        kv_store = server_state.kv_store\n","        etype_offset = partition_book.local_etype_offset\n","        # See NOTE 1\n","        if self.prob is not None:\n","            probs = [\n","                kv_store.data_store[key] if key != \"\" else None\n","                for key in self.prob\n","            ]\n","        else:\n","            probs = None\n","        global_src, global_dst, global_eids = _sample_etype_neighbors(\n","            local_g,\n","            partition_book,\n","            self.seed_nodes,\n","            etype_offset,\n","            self.fan_out,\n","            self.edge_dir,\n","            probs,\n","            self.replace,\n","            self.etype_sorted,\n","        )\n","        return SubgraphResponse(global_src, global_dst, global_eids)\n","\n","\n","class EdgesRequest(Request):\n","    \"\"\"Edges Request\"\"\"\n","\n","    def __init__(self, edge_ids, order_id):\n","        self.edge_ids = edge_ids\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.edge_ids, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.edge_ids, self.order_id\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        global_src, global_dst = _find_edges(\n","            local_g, partition_book, self.edge_ids\n","        )\n","\n","        return FindEdgeResponse(global_src, global_dst, self.order_id)\n","\n","\n","class InDegreeRequest(Request):\n","    \"\"\"In-degree Request\"\"\"\n","\n","    def __init__(self, n, order_id):\n","        self.n = n\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.n, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.n, self.order_id\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        deg = _in_degrees(local_g, partition_book, self.n)\n","\n","        return InDegreeResponse(deg, self.order_id)\n","\n","\n","class InDegreeResponse(Response):\n","    \"\"\"The response for in-degree\"\"\"\n","\n","    def __init__(self, deg, order_id):\n","        self.val = deg\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.val, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.val, self.order_id\n","\n","\n","class OutDegreeRequest(Request):\n","    \"\"\"Out-degree Request\"\"\"\n","\n","    def __init__(self, n, order_id):\n","        self.n = n\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.n, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.n, self.order_id\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        deg = _out_degrees(local_g, partition_book, self.n)\n","\n","        return OutDegreeResponse(deg, self.order_id)\n","\n","\n","class OutDegreeResponse(Response):\n","    \"\"\"The response for out-degree\"\"\"\n","\n","    def __init__(self, deg, order_id):\n","        self.val = deg\n","        self.order_id = order_id\n","\n","    def __setstate__(self, state):\n","        self.val, self.order_id = state\n","\n","    def __getstate__(self):\n","        return self.val, self.order_id\n","\n","\n","class InSubgraphRequest(Request):\n","    \"\"\"InSubgraph Request\"\"\"\n","\n","    def __init__(self, nodes):\n","        self.seed_nodes = nodes\n","\n","    def __setstate__(self, state):\n","        self.seed_nodes = state\n","\n","    def __getstate__(self):\n","        return self.seed_nodes\n","\n","    def process_request(self, server_state):\n","        local_g = server_state.graph\n","        partition_book = server_state.partition_book\n","        global_src, global_dst, global_eids = _in_subgraph(\n","            local_g, partition_book, self.seed_nodes\n","        )\n","        return SubgraphResponse(global_src, global_dst, global_eids)\n","\n","\n","def merge_graphs(res_list, num_nodes):\n","    \"\"\"Merge request from multiple servers\"\"\"\n","    if len(res_list) > 1:\n","        srcs = []\n","        dsts = []\n","        eids = []\n","        for res in res_list:\n","            srcs.append(res.global_src)\n","            dsts.append(res.global_dst)\n","            eids.append(res.global_eids)\n","        src_tensor = F.cat(srcs, 0)\n","        dst_tensor = F.cat(dsts, 0)\n","        eid_tensor = F.cat(eids, 0)\n","    else:\n","        src_tensor = res_list[0].global_src\n","        dst_tensor = res_list[0].global_dst\n","        eid_tensor = res_list[0].global_eids\n","    g = graph((src_tensor, dst_tensor), num_nodes=num_nodes)\n","    g.edata[EID] = eid_tensor\n","    return g\n","\n","\n","LocalSampledGraph = namedtuple(\n","    \"LocalSampledGraph\", \"global_src global_dst global_eids\"\n",")\n","\n","\n","def _distributed_access(g, nodes, issue_remote_req, local_access):\n","    \"\"\"A routine that fetches local neighborhood of nodes from the distributed graph.\n","\n","    The local neighborhood of some nodes are stored in the local machine and the other\n","    nodes have their neighborhood on remote machines. This code will issue remote\n","    access requests first before fetching data from the local machine. In the end,\n","    we combine the data from the local machine and remote machines.\n","    In this way, we can hide the latency of accessing data on remote machines.\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph\n","    nodes : tensor\n","        The nodes whose neighborhood are to be fetched.\n","    issue_remote_req : callable\n","        The function that issues requests to access remote data.\n","    local_access : callable\n","        The function that reads data on the local machine.\n","\n","    Returns\n","    -------\n","    DGLGraph\n","        The subgraph that contains the neighborhoods of all input nodes.\n","    \"\"\"\n","    req_list = []\n","    partition_book = g.get_partition_book()\n","    nodes = toindex(nodes).tousertensor()\n","    partition_id = partition_book.nid2partid(nodes)\n","    local_nids = None\n","    data1 = \"0\"\n","    data2 = \"0\"\n","    for pid in range(partition_book.num_partitions()):\n","        node_id = F.boolean_mask(nodes, partition_id == pid)\n","        # We optimize the sampling on a local partition if the server and the client\n","        # run on the same machine. With a good partitioning, most of the seed nodes\n","        # should reside in the local partition. If the server and the client\n","        # are not co-located, the client doesn't have a local partition.\n","        if pid == partition_book.partid and g.local_partition is not None:\n","            assert local_nids is None\n","            local_nids = node_id\n","            data1 = str(len(local_nids))\n","        elif len(node_id) != 0:\n","            req = issue_remote_req(node_id)\n","            req_list.append((pid, req))\n","            data2 = str(len(node_id))\n","\n","\n","    f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/bfs_{}_loc_rem.txt\".format(\"1000\"),'a+')\n","    data = str(len(nodes))\n","    f.write(data+\"\\n\")\n","    f.write(data1+\"\\n\")\n","    f.write(data2+\"\\n\")\n","    f.close()\n","\n","\n","    # send requests to the remote machine.\n","\n","    msgseq2pos = None\n","\n","    start = time.time()\n","\n","    if len(req_list) > 0:\n","        msgseq2pos = send_requests_to_machine(req_list)\n","\n","    # sample neighbors for the nodes in the local partition.\n","    local_start_time = time.time()\n","    res_list = []\n","    if local_nids is not None:\n","        src, dst, eids = local_access(\n","            g.local_partition, partition_book, local_nids\n","        )\n","        res_list.append(LocalSampledGraph(src, dst, eids))\n","\n","    local_end_time = time.time()\n","    local_sample_time = local_end_time - local_start_time\n","\n","    # receive responses from remote machines.\n","    if msgseq2pos is not None:\n","        results = recv_responses(msgseq2pos)\n","        res_list.extend(results)\n","\n","    end = time.time()\n","    sample_time = end-start\n","    f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/bfs_{}_loc_rem.txt\".format(\"1000\"),'a+')\n","    data_local = str(local_sample_time)\n","    data = str(sample_time)\n","    f.write(data_local+\"\\n\"+data+\"\\n\")\n","    f.close()\n","\n","    sampled_graph = merge_graphs(res_list, g.number_of_nodes())\n","    return sampled_graph\n","\n","def _dfs_distributed_access(g, nodes, issue_remote_req, local_access, fan_outs):\n","\n","    partition_book = g.get_partition_book()\n","    local_nids = None\n","    sampledGraphs = []\n","    for i in range(len(fan_outs)):\n","      sampledGraphs.append([])\n","    layer_idx = 0\n","    exist_flag = True\n","    num_while_loop = 1\n","    bucket = []\n","    for i in range(len(fan_outs)-1):\n","        bucket.append([])\n","\n","    while(num_while_loop<=len(fan_outs)):\n","      exist_flag = False\n","      RemoteSampledGraphs = []\n","      LocalSampledGraphs = []\n","      req_list = []\n","\n","      # check all partition ids of nodes\n","      local_nids, remote_list = _check_location(g, nodes)\n","\n","      # send requests to the remote machine.\n","      msgseq2pos = None\n","\n","    #   start = time.time()\n","\n","      if len(remote_list) > 0:\n","          for remote in remote_list:\n","            pid = remote[0]\n","            node_ids_each_machine_tensor = remote[1]\n","            req = issue_remote_req(node_ids_each_machine_tensor, bucket, layer_idx)\n","            req_list.append((pid, req))\n","          msgseq2pos = send_requests_to_machine(req_list)\n","\n","      # sample neighbors for the nodes in the local partition.\n","      local_start_time = time.time()\n","      local_bucket = []\n","      if local_nids is not None and len(local_nids)!=0:\n","          LocalSampledGraphs, local_bucket = local_access(\n","              g.local_partition, partition_book, local_nids, bucket, layer_idx\n","          )\n","\n","      local_end_time = time.time()\n","      local_sample_time = local_end_time - local_start_time\n","\n","\n","      bucket = []\n","      for i in range(len(fan_outs)-1):\n","          bucket.append([])\n","\n","      # receive responses from remote machines.\n","\n","      remote_buckets = []\n","      if msgseq2pos is not None:\n","          res = recv_responses(msgseq2pos)\n","          for items in reversed(res):\n","              RemoteSampledGraphs = items.__getstate__()[0]\n","              remote_bucket = items.__getstate__()[1]\n","              for i in reversed(range(len(RemoteSampledGraphs))):\n","                sampledGraphs[layer_idx+i].append(RemoteSampledGraphs[i])\n","\n","              for i in range(len(bucket)):\n","                # print(\"part0;_dfs_distributed_access:len(remote_bucket[i])\",i,\":\",len(remote_bucket[i]))\n","                bucket[i].extend(remote_bucket[i])\n","\n","    #   end = time.time()\n","    #   sample_time = end-start\n","    #   f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/dfs_{}_loc_rem.txt\".format(\"1000\"),'a+')\n","    #   data_local = str(local_sample_time)\n","    #   data = str(sample_time)\n","    #   f.write(data_local+\"\\n\"+data+\"\\n\")\n","    #   f.close()\n","\n","      # concatenate results\n","\n","\n","      for i in reversed(range(len(LocalSampledGraphs))):\n","          sampledGraphs[layer_idx+i].append(LocalSampledGraphs[i])\n","\n","      if len(local_bucket)!=0:\n","        for i in range(len(bucket)):\n","            bucket[i].extend(local_bucket[i])\n","\n","      # check bucket is empty or not\n","      for i, layer in enumerate(bucket):\n","        if len(layer)!=0:\n","          exist_flag =True\n","\n","          # change nodes of first bucket\n","          nodes = layer\n","        #   print(\"part0;_dfs_distributed_access:len(nodes)\",i,\":\",len(nodes))\n","          layer_idx = i+1\n","          break\n","      # bucket is empty so break while loop\n","      if exist_flag==False:\n","\n","        break\n","      num_while_loop = num_while_loop+1\n","\n","    # f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/dfs_num_requests.txt\",'a+')\n","    # data = str(num_while_loop)\n","    # f.write(data+\"\\n\")\n","    # f.close()\n","\n","    # merge graphs\n","    sampled_graphs = []\n","    for i in range(len(sampledGraphs)):\n","        sampled_graph = merge_graphs(sampledGraphs[i], g.number_of_nodes())\n","        sampled_graphs.append(sampled_graph)\n","\n","    return sampled_graphs\n","\n","def _frontier_to_heterogeneous_graph(g, frontier, gpb):\n","    # We need to handle empty frontiers correctly.\n","    if frontier.number_of_edges() == 0:\n","        data_dict = {\n","            etype: (np.zeros(0), np.zeros(0)) for etype in g.canonical_etypes\n","        }\n","        return heterograph(\n","            data_dict,\n","            {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes},\n","            idtype=g.idtype,\n","        )\n","\n","    etype_ids, frontier.edata[EID] = gpb.map_to_per_etype(frontier.edata[EID])\n","    src, dst = frontier.edges()\n","    etype_ids, idx = F.sort_1d(etype_ids)\n","    src, dst = F.gather_row(src, idx), F.gather_row(dst, idx)\n","    eid = F.gather_row(frontier.edata[EID], idx)\n","    _, src = gpb.map_to_per_ntype(src)\n","    _, dst = gpb.map_to_per_ntype(dst)\n","\n","    data_dict = dict()\n","    edge_ids = {}\n","    for etid, etype in enumerate(g.canonical_etypes):\n","        type_idx = etype_ids == etid\n","        if F.sum(type_idx, 0) > 0:\n","            data_dict[etype] = (\n","                F.boolean_mask(src, type_idx),\n","                F.boolean_mask(dst, type_idx),\n","            )\n","            edge_ids[etype] = F.boolean_mask(eid, type_idx)\n","    hg = heterograph(\n","        data_dict,\n","        {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes},\n","        idtype=g.idtype,\n","    )\n","\n","    for etype in edge_ids:\n","        hg.edges[etype].data[EID] = edge_ids[etype]\n","    return hg\n","\n","\n","def sample_etype_neighbors(\n","    g,\n","    nodes,\n","    fanout,\n","    edge_dir=\"in\",\n","    prob=None,\n","    replace=False,\n","    etype_sorted=True,\n","):\n","    \"\"\"Sample from the neighbors of the given nodes from a distributed graph.\n","\n","    For each node, a number of inbound (or outbound when ``edge_dir == 'out'``) edges\n","    will be randomly chosen.  The returned graph will contain all the nodes in the\n","    original graph, but only the sampled edges.\n","\n","    Node/edge features are not preserved. The original IDs of\n","    the sampled edges are stored as the `dgl.EID` feature in the returned graph.\n","\n","    This function assumes the input is a homogeneous ``DGLGraph`` with the edges\n","    ordered by their edge types. The sampled subgraph is also\n","    stored in the homogeneous graph format. That is, all nodes and edges are assigned\n","    with unique IDs (in contrast, we typically use a type name and a node/edge ID to\n","    identify a node or an edge in ``DGLGraph``). We refer to this type of IDs\n","    as *homogeneous ID*.\n","    Users can use :func:`dgl.distributed.GraphPartitionBook.map_to_per_ntype`\n","    and :func:`dgl.distributed.GraphPartitionBook.map_to_per_etype`\n","    to identify their node/edge types and node/edge IDs of that type.\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph..\n","    nodes : tensor or dict\n","        Node IDs to sample neighbors from. If it's a dict, it should contain only\n","        one key-value pair to make this API consistent with dgl.sampling.sample_neighbors.\n","    fanout : int or dict[etype, int]\n","        The number of edges to be sampled for each node per edge type.  If an integer\n","        is given, DGL assumes that the same fanout is applied to every edge type.\n","\n","        If -1 is given, all of the neighbors will be selected.\n","    edge_dir : str, optional\n","        Determines whether to sample inbound or outbound edges.\n","\n","        Can take either ``in`` for inbound edges or ``out`` for outbound edges.\n","    prob : str, optional\n","        Feature name used as the (unnormalized) probabilities associated with each\n","        neighboring edge of a node.  The feature must have only one element for each\n","        edge.\n","\n","        The features must be non-negative floats, and the sum of the features of\n","        inbound/outbound edges for every node must be positive (though they don't have\n","        to sum up to one).  Otherwise, the result will be undefined.\n","    replace : bool, optional\n","        If True, sample with replacement.\n","\n","        When sampling with replacement, the sampled subgraph could have parallel edges.\n","\n","        For sampling without replacement, if fanout > the number of neighbors, all the\n","        neighbors are sampled. If fanout == -1, all neighbors are collected.\n","    etype_sorted : bool, optional\n","        Indicates whether etypes are sorted.\n","\n","    Returns\n","    -------\n","    DGLGraph\n","        A sampled subgraph containing only the sampled neighboring edges.  It is on CPU.\n","    \"\"\"\n","    if isinstance(fanout, int):\n","        fanout = F.full_1d(len(g.canonical_etypes), fanout, F.int64, F.cpu())\n","    else:\n","        etype_ids = {etype: i for i, etype in enumerate(g.canonical_etypes)}\n","        fanout_array = [None] * len(g.canonical_etypes)\n","        for etype, v in fanout.items():\n","            c_etype = g.to_canonical_etype(etype)\n","            fanout_array[etype_ids[c_etype]] = v\n","        assert all(v is not None for v in fanout_array), (\n","            \"Not all etypes have valid fanout. Please make sure passed-in \"\n","            \"fanout in dict includes all the etypes in graph. Passed-in \"\n","            f\"fanout: {fanout}, graph etypes: {g.canonical_etypes}.\"\n","        )\n","        fanout = F.tensor(fanout_array, dtype=F.int64)\n","\n","    gpb = g.get_partition_book()\n","    if isinstance(nodes, dict):\n","        homo_nids = []\n","        for ntype in nodes.keys():\n","            assert (\n","                ntype in g.ntypes\n","            ), \"The sampled node type {} does not exist in the input graph\".format(\n","                ntype\n","            )\n","            if F.is_tensor(nodes[ntype]):\n","                typed_nodes = nodes[ntype]\n","            else:\n","                typed_nodes = toindex(nodes[ntype]).tousertensor()\n","            homo_nids.append(gpb.map_to_homo_nid(typed_nodes, ntype))\n","        nodes = F.cat(homo_nids, 0)\n","\n","    def issue_remote_req(node_ids):\n","        if prob is not None:\n","            # See NOTE 1\n","            _prob = [\n","                # NOTE (BarclayII)\n","                # Currently DistGraph.edges[] does not accept canonical etype.\n","                g.edges[etype].data[prob].kvstore_key\n","                if prob in g.edges[etype].data\n","                else \"\"\n","                for etype in g.canonical_etypes\n","            ]\n","        else:\n","            _prob = None\n","        return SamplingRequestEtype(\n","            node_ids,\n","            fanout,\n","            edge_dir=edge_dir,\n","            prob=_prob,\n","            replace=replace,\n","            etype_sorted=etype_sorted,\n","        )\n","\n","    def local_access(local_g, partition_book, local_nids):\n","        etype_offset = gpb.local_etype_offset\n","        # See NOTE 1\n","        if prob is None:\n","            _prob = None\n","        else:\n","            _prob = [\n","                g.edges[etype].data[prob].local_partition\n","                if prob in g.edges[etype].data\n","                else None\n","                for etype in g.canonical_etypes\n","            ]\n","        return _sample_etype_neighbors(\n","            local_g,\n","            partition_book,\n","            local_nids,\n","            etype_offset,\n","            fanout,\n","            edge_dir,\n","            _prob,\n","            replace,\n","            etype_sorted=etype_sorted,\n","        )\n","\n","    frontier = _distributed_access(g, nodes, issue_remote_req, local_access)\n","    if not gpb.is_homogeneous:\n","        return _frontier_to_heterogeneous_graph(g, frontier, gpb)\n","    else:\n","        return frontier\n","\n","\n","def sample_neighbors(g, nodes, fanout, edge_dir=\"in\", prob=None, replace=False):\n","    \"\"\"Sample from the neighbors of the given nodes from a distributed graph.\n","\n","    For each node, a number of inbound (or outbound when ``edge_dir == 'out'``) edges\n","    will be randomly chosen.  The returned graph will contain all the nodes in the\n","    original graph, but only the sampled edges.\n","\n","    Node/edge features are not preserved. The original IDs of\n","    the sampled edges are stored as the `dgl.EID` feature in the returned graph.\n","\n","    For heterogeneous graphs, ``nodes`` is a dictionary whose key is node type\n","    and the value is type-specific node IDs.\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph..\n","    nodes : tensor or dict\n","        Node IDs to sample neighbors from. If it's a dict, it should contain only\n","        one key-value pair to make this API consistent with dgl.sampling.sample_neighbors.\n","    fanout : int\n","        The number of edges to be sampled for each node.\n","\n","        If -1 is given, all of the neighbors will be selected.\n","    edge_dir : str, optional\n","        Determines whether to sample inbound or outbound edges.\n","\n","        Can take either ``in`` for inbound edges or ``out`` for outbound edges.\n","    prob : str, optional\n","        Feature name used as the (unnormalized) probabilities associated with each\n","        neighboring edge of a node.  The feature must have only one element for each\n","        edge.\n","\n","        The features must be non-negative floats, and the sum of the features of\n","        inbound/outbound edges for every node must be positive (though they don't have\n","        to sum up to one).  Otherwise, the result will be undefined.\n","    replace : bool, optional\n","        If True, sample with replacement.\n","\n","        When sampling with replacement, the sampled subgraph could have parallel edges.\n","\n","        For sampling without replacement, if fanout > the number of neighbors, all the\n","        neighbors are sampled. If fanout == -1, all neighbors are collected.\n","\n","    Returns\n","    -------\n","    DGLGraph\n","        A sampled subgraph containing only the sampled neighboring edges.  It is on CPU.\n","    \"\"\"\n","    gpb = g.get_partition_book()\n","    if not gpb.is_homogeneous:\n","        assert isinstance(nodes, dict)\n","        homo_nids = []\n","        for ntype in nodes:\n","            assert (\n","                ntype in g.ntypes\n","            ), \"The sampled node type does not exist in the input graph\"\n","            if F.is_tensor(nodes[ntype]):\n","                typed_nodes = nodes[ntype]\n","            else:\n","                typed_nodes = toindex(nodes[ntype]).tousertensor()\n","            homo_nids.append(gpb.map_to_homo_nid(typed_nodes, ntype))\n","        nodes = F.cat(homo_nids, 0)\n","    elif isinstance(nodes, dict):\n","        assert len(nodes) == 1\n","        nodes = list(nodes.values())[0]\n","\n","    def issue_remote_req(node_ids):\n","        if prob is not None:\n","            # See NOTE 1\n","            _prob = g.edata[prob].kvstore_key\n","        else:\n","            _prob = None\n","        return SamplingRequest(\n","            node_ids, fanout, edge_dir=edge_dir, prob=_prob, replace=replace\n","        )\n","\n","    def local_access(local_g, partition_book, local_nids):\n","        # See NOTE 1\n","        _prob = (\n","            [g.edata[prob].local_partition] if prob is not None else None\n","        )\n","        return _sample_neighbors(\n","            local_g,\n","            partition_book,\n","            local_nids,\n","            fanout,\n","            edge_dir,\n","            _prob,\n","            replace,\n","        )\n","\n","    frontier = _distributed_access(g, nodes, issue_remote_req, local_access)\n","    if not gpb.is_homogeneous:\n","        return _frontier_to_heterogeneous_graph(g, frontier, gpb)\n","    else:\n","        return frontier\n","\n","def _check_location(g, nodes):\n","    remote_list = []\n","    partition_book = g.get_partition_book()\n","    nodes_tensor = toindex(nodes).tousertensor()\n","    # global node id to partition id\n","    # nodes가 위치한 partition의 id 모음을 반환\n","    partition_ids_of_nodes_tensor = partition_book.nid2partid(nodes)\n","    local_nids = None\n","    data1 = \"0\"\n","    data2 = \"0\"\n","    #check partition ids of nodes\n","    for pid in range(partition_book.num_partitions()):\n","      # node_ids_each_machine_tensor = nodes_tensor[partition_ids_of_nodes_tensor==pid]\n","      # node 중 각 머신의 pid에 해당하는 부분만 가지고옴\n","      node_ids_each_machine_tensor = F.boolean_mask(nodes_tensor, partition_ids_of_nodes_tensor==pid)\n","      if pid == partition_book.partid and g.local_partition is not None:\n","          assert local_nids is None\n","          local_nids = node_ids_each_machine_tensor\n","          data1 = str(len(local_nids))\n","\n","      elif len(node_ids_each_machine_tensor) != 0:\n","          # req = issue_remote_req(node_ids_each_machine_tensor)\n","          remote_list.append((pid, node_ids_each_machine_tensor))\n","          data2 = str(len(node_ids_each_machine_tensor))\n","\n","\n","    f = open(\"/home/van/disney_workplace/dgl/examples/pytorch/graphsage/dist/dfs_{}_loc_rem.txt\".format(\"1000\"),'a+')\n","    data = str(len(nodes))\n","    f.write(data+\"\\n\")\n","    f.write(data1+\"\\n\")\n","    f.write(data2+\"\\n\")\n","    f.close()\n","\n","    return local_nids, remote_list\n","\n","def _check_location_local(g, partition_book, nodes):\n","    remote_list = []\n","    nodes_tensor = toindex(nodes).tousertensor()\n","    # global node id to partition id\n","    # nodes가 위치한 partition의 id 모음을 반환\n","    partition_ids_of_nodes_tensor = partition_book.nid2partid(nodes)\n","\n","    #check partition ids of nodes\n","    local_nids = nodes_tensor[partition_ids_of_nodes_tensor==partition_book.partid]\n","    remote_nids_tensor = nodes_tensor[partition_ids_of_nodes_tensor!=partition_book.partid]\n","    # local_nids = F.boolean_mask(nodes_tensor, partition_ids_of_nodes_tensor==partition_book.partid)\n","    # remote_nids_tensor = F.boolean_mask(nodes_tensor, partition_ids_of_nodes_tensor!=partition_book.partid)\n","    if len(remote_nids_tensor)!=0:\n","      remote_list = remote_nids_tensor.tolist()\n","\n","    return local_nids, remote_list\n","\n","def _check_local(partition_book, nodes):\n","    # bucket 내의 node들을 check\n","    remote_list = []\n","    nodes_tensor = toindex(nodes).tousertensor()\n","    partition_ids_of_nodes_tensor = partition_book.nid2partid(nodes)\n","    local_ids = None\n","    #check partition ids of nodes\n","    local_ids = F.boolean_mask(nodes_tensor, partition_ids_of_nodes_tensor==partition_book.partid)\n","\n","    return local_ids\n","\n","def df_sample_neighbors(g, nodes, fanouts, edge_dir=\"in\", prob=None, replace=False):\n","\n","    # nodes는 list이면 됨\n","    gpb = g.get_partition_book()\n","    if not gpb.is_homogeneous:\n","        print(\"Only homogeneous graphs are supported by dfs\")\n","        pass\n","    elif isinstance(nodes, dict):\n","        assert len(nodes) == 1\n","        nodes = list(nodes.values())[0]\n","\n","    def issue_remote_req(node_ids_each_machine_tensor, bucket, layer_idx):\n","        # node_ids tensor; 글로벌 아이디\n","        if prob is not None:\n","            # See NOTE 1\n","            _prob = g.edata[prob].kvstore_key\n","        else:\n","            _prob = None\n","        return DFSamplingRequest(\n","            node_ids_each_machine_tensor, fanouts, edge_dir=edge_dir, prob=_prob, replace=replace, bucket=bucket, layer_idx=layer_idx\n","        )\n","\n","    def local_access(local_g, partition_book, local_nids, bucket, layer_idx):\n","        # See NOTE 1\n","        _prob = [g.edata[prob].local_partition] if prob is not None else None\n","\n","        return _df_sample_neighbors(\n","            local_g,\n","            partition_book,\n","            local_nids,\n","            fanouts,\n","            edge_dir,\n","            _prob,\n","            replace,\n","            bucket,\n","            layer_idx\n","        )\n","\n","    frontiers = _dfs_distributed_access(g, nodes, issue_remote_req, local_access, fanouts)\n","    return frontiers\n","\n","def _distributed_edge_access(g, edges, issue_remote_req, local_access):\n","    \"\"\"A routine that fetches local edges from distributed graph.\n","\n","    The source and destination nodes of local edges are stored in the local\n","    machine and others are stored on remote machines. This code will issue\n","    remote access requests first before fetching data from the local machine.\n","    In the end, we combine the data from the local machine and remote machines.\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph\n","    edges : tensor\n","        The edges to find their source and destination nodes.\n","    issue_remote_req : callable\n","        The function that issues requests to access remote data.\n","    local_access : callable\n","        The function that reads data on the local machine.\n","\n","    Returns\n","    -------\n","    tensor\n","        The source node ID array.\n","    tensor\n","        The destination node ID array.\n","    \"\"\"\n","    req_list = []\n","    partition_book = g.get_partition_book()\n","    edges = toindex(edges).tousertensor()\n","    partition_id = partition_book.eid2partid(edges)\n","    local_eids = None\n","    reorder_idx = []\n","    for pid in range(partition_book.num_partitions()):\n","        mask = partition_id == pid\n","        edge_id = F.boolean_mask(edges, mask)\n","        reorder_idx.append(F.nonzero_1d(mask))\n","        if pid == partition_book.partid and g.local_partition is not None:\n","            assert local_eids is None\n","            local_eids = edge_id\n","        elif len(edge_id) != 0:\n","            req = issue_remote_req(edge_id, pid)\n","            req_list.append((pid, req))\n","\n","    # send requests to the remote machine.\n","    msgseq2pos = None\n","    if len(req_list) > 0:\n","        msgseq2pos = send_requests_to_machine(req_list)\n","\n","    # handle edges in local partition.\n","    src_ids = F.zeros_like(edges)\n","    dst_ids = F.zeros_like(edges)\n","    if local_eids is not None:\n","        src, dst = local_access(g.local_partition, partition_book, local_eids)\n","        src_ids = F.scatter_row(\n","            src_ids, reorder_idx[partition_book.partid], src\n","        )\n","        dst_ids = F.scatter_row(\n","            dst_ids, reorder_idx[partition_book.partid], dst\n","        )\n","\n","    # receive responses from remote machines.\n","    if msgseq2pos is not None:\n","        results = recv_responses(msgseq2pos)\n","        for result in results:\n","            src = result.global_src\n","            dst = result.global_dst\n","            src_ids = F.scatter_row(src_ids, reorder_idx[result.order_id], src)\n","            dst_ids = F.scatter_row(dst_ids, reorder_idx[result.order_id], dst)\n","    return src_ids, dst_ids\n","\n","\n","def find_edges(g, edge_ids):\n","    \"\"\"Given an edge ID array, return the source and destination\n","    node ID array ``s`` and ``d`` from a distributed graph.\n","    ``s[i]`` and ``d[i]`` are source and destination node ID for\n","    edge ``eid[i]``.\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph.\n","    edges : tensor\n","        The edge ID array.\n","\n","    Returns\n","    -------\n","    tensor\n","        The source node ID array.\n","    tensor\n","        The destination node ID array.\n","    \"\"\"\n","\n","    def issue_remote_req(edge_ids, order_id):\n","        return EdgesRequest(edge_ids, order_id)\n","\n","    def local_access(local_g, partition_book, edge_ids):\n","        return _find_edges(local_g, partition_book, edge_ids)\n","\n","    return _distributed_edge_access(g, edge_ids, issue_remote_req, local_access)\n","\n","\n","def in_subgraph(g, nodes):\n","    \"\"\"Return the subgraph induced on the inbound edges of the given nodes.\n","\n","    The subgraph keeps the same type schema and all the nodes are preserved regardless\n","    of whether they have an edge or not.\n","\n","    Node/edge features are not preserved. The original IDs of\n","    the extracted edges are stored as the `dgl.EID` feature in the returned graph.\n","\n","    For now, we only support the input graph with one node type and one edge type.\n","\n","\n","    Parameters\n","    ----------\n","    g : DistGraph\n","        The distributed graph structure.\n","    nodes : tensor or dict\n","        Node ids to sample neighbors from.\n","\n","    Returns\n","    -------\n","    DGLGraph\n","        The subgraph.\n","\n","        One can retrieve the mapping from subgraph edge ID to parent\n","        edge ID via ``dgl.EID`` edge features of the subgraph.\n","    \"\"\"\n","    if isinstance(nodes, dict):\n","        assert (\n","            len(nodes) == 1\n","        ), \"The distributed in_subgraph only supports one node type for now.\"\n","        nodes = list(nodes.values())[0]\n","\n","    def issue_remote_req(node_ids):\n","        return InSubgraphRequest(node_ids)\n","\n","    def local_access(local_g, partition_book, local_nids):\n","        return _in_subgraph(local_g, partition_book, local_nids)\n","\n","    return _distributed_access(g, nodes, issue_remote_req, local_access)\n","\n","\n","def _distributed_get_node_property(g, n, issue_remote_req, local_access):\n","    req_list = []\n","    partition_book = g.get_partition_book()\n","    n = toindex(n).tousertensor()\n","    partition_id = partition_book.nid2partid(n)\n","    local_nids = None\n","    reorder_idx = []\n","    for pid in range(partition_book.num_partitions()):\n","        mask = partition_id == pid\n","        nid = F.boolean_mask(n, mask)\n","        reorder_idx.append(F.nonzero_1d(mask))\n","        if pid == partition_book.partid and g.local_partition is not None:\n","            assert local_nids is None\n","            local_nids = nid\n","        elif len(nid) != 0:\n","            req = issue_remote_req(nid, pid)\n","            req_list.append((pid, req))\n","\n","    # send requests to the remote machine.\n","    msgseq2pos = None\n","    if len(req_list) > 0:\n","        msgseq2pos = send_requests_to_machine(req_list)\n","\n","    # handle edges in local partition.\n","    vals = None\n","    if local_nids is not None:\n","        local_vals = local_access(g.local_partition, partition_book, local_nids)\n","        shape = list(F.shape(local_vals))\n","        shape[0] = len(n)\n","        vals = F.zeros(shape, F.dtype(local_vals), F.cpu())\n","        vals = F.scatter_row(\n","            vals, reorder_idx[partition_book.partid], local_vals\n","        )\n","\n","    # receive responses from remote machines.\n","    if msgseq2pos is not None:\n","        results = recv_responses(msgseq2pos)\n","        if len(results) > 0 and vals is None:\n","            shape = list(F.shape(results[0].val))\n","            shape[0] = len(n)\n","            vals = F.zeros(shape, F.dtype(results[0].val), F.cpu())\n","        for result in results:\n","            val = result.val\n","            vals = F.scatter_row(vals, reorder_idx[result.order_id], val)\n","    return vals\n","\n","\n","def in_degrees(g, v):\n","    \"\"\"Get in-degrees\"\"\"\n","\n","    def issue_remote_req(v, order_id):\n","        return InDegreeRequest(v, order_id)\n","\n","    def local_access(local_g, partition_book, v):\n","        return _in_degrees(local_g, partition_book, v)\n","\n","    return _distributed_get_node_property(g, v, issue_remote_req, local_access)\n","\n","\n","def out_degrees(g, u):\n","    \"\"\"Get out-degrees\"\"\"\n","\n","    def issue_remote_req(u, order_id):\n","        return OutDegreeRequest(u, order_id)\n","\n","    def local_access(local_g, partition_book, u):\n","        return _out_degrees(local_g, partition_book, u)\n","\n","    return _distributed_get_node_property(g, u, issue_remote_req, local_access)\n","\n","\n","register_service(SAMPLING_SERVICE_ID, SamplingRequest, SubgraphResponse)\n","register_service(EDGES_SERVICE_ID, EdgesRequest, FindEdgeResponse)\n","register_service(INSUBGRAPH_SERVICE_ID, InSubgraphRequest, SubgraphResponse)\n","register_service(OUTDEGREE_SERVICE_ID, OutDegreeRequest, OutDegreeResponse)\n","register_service(INDEGREE_SERVICE_ID, InDegreeRequest, InDegreeResponse)\n","register_service(\n","    ETYPE_SAMPLING_SERVICE_ID, SamplingRequestEtype, SubgraphResponse\n",")\n","register_service(DFSAMPLING_SERVICE_ID, DFSamplingRequest, DFSubgraphResponse)"]}]}